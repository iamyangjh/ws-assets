{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "################################################################################\n#Licensed Materials - Property of IBM\n#(C) Copyright IBM Corp. 2019\n#US Government Users Restricted Rights - Use, duplication disclosure restricted\n#by GSA ADP Schedule Contract with IBM Corp.\n################################################################################\n\nThe auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for Watson Studio Auto-generated Notebook (\"License Terms\"), such agreements located in the link below.\nSpecifically, the Source Components and Sample Materials clause included in the License Information document for\nWatson Studio Auto-generated Notebook applies to the auto-generated notebooks. \nBy downloading, copying, accessing, or otherwise using the materials, you agree to the License Terms.\nhttp://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BHU2B7&title=IBM%20Watson%20Studio%20Auto-generated%20Notebook%20V2.1\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## IBM AutoAI Auto-Generated Notebook v1.10.6\n### Representing Pipeline: P2 from run ca7f34b4-7e06-470c-8ad2-405800453365\n\n**Note**: Notebook code generated using AutoAI will execute successfully.  If code is modified or reordered, there is no guarantee it will successfully execute, please read our documentation for more information https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .\n\nBefore modifying the pipeline or trying to re-fit the pipeline, consider:\nThe notebook converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\nThe known_values_list is passed by reference and populated with categorical values during fit of the preprocessing pipeline.  Delete its members before re-fitting.\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 1. Set Up"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import sklearn\ntry:\n    import xgboost\nexcept:\n    print('xgboost, if needed, will be installed and imported later')\ntry:\n    import lightgbm\nexcept:\n    print('lightgbm, if needed, will be installed and imported later')\nfrom sklearn.cluster import FeatureAgglomeration\nimport numpy\nfrom numpy import nan, dtype, mean\nimport autoai_libs\nfrom autoai_libs.sklearn.custom_scorers import CustomScorers\nimport sklearn.ensemble\nfrom autoai_libs.cognito.transforms.transform_utils import TExtras, FC\nfrom autoai_libs.transformers.exportable import *\nfrom autoai_libs.utils.exportable_utils import *\nfrom sklearn.pipeline import Pipeline\nknown_values_list=[]\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 2. Compose Pipeline"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# @hidden_cell\n#THIS CELL HAS hmac CREDENTIALS TO READ YOUR PROJECT'S CLOUD OBJECT STORAGE BUCKET.  CONSIDER DELETING THE KEYS PRIOR TO SHARING THE NOTEBOOK.\n#Load pipeline while attempting to automatically import modules and install missing packages as necessary.\nretries = 0\nsuccessful = False\nfailed_retries = 0\nwhile retries < 100 and failed_retries < 10 and not successful:\n    retries += 1\n    failed_retries += 1\n    try:\n        #\n        # composing steps for toplevel Pipeline\n        #\n        _input_metadata = {'run_uid': 'ca7f34b4-7e06-470c-8ad2-405800453365', 'pn': 'P2', 'data_source': '', 'target_label_name': 'CHURN', 'learning_type': 'classification', 'optimization_metric': 'roc_auc', 'random_state': 33, 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'data_provenance': {'input_data': [{'connection': {'access_key_id': '4df72a10fe89410c98832ce60dfa7394', 'secret_access_key': '7f9be60f8531a0d239f07fcb08bde0acb240942cc0f5b60e', 'endpoint_url': 'https://s3-api.us-geo.objectstorage.softlayer.net'}, 'id': '1', 'location': {'type': 's3', 'bucket': 'yjhautoai-donotdelete-pr-3ijmn7gdq2bkmt', 'path': 'customer_churn.csv'}, 'type': 's3'}]}, 'pos_label': 'T'}\n        steps=[]\n        steps.append(('column_selector', autoai_libs.transformers.exportable.ColumnSelector(activate_flag=True,\n                columns_indices_list=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])))\n        #\n        # composing steps for preprocessor Pipeline\n        #\n        preprocessor__input_metadata = None\n        preprocessor_steps=[]\n        #\n        # composing steps for preprocessor_features FeatureUnion\n        #\n        preprocessor_features_transformer_list=[]\n        #\n        # composing steps for preprocessor_features_categorical Pipeline\n        #\n        preprocessor_features_categorical__input_metadata = None\n        preprocessor_features_categorical_steps=[]\n        preprocessor_features_categorical_steps.append(('cat_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[0, 1, 2, 4, 9, 10, 11, 12, 14])))\n        preprocessor_features_categorical_steps.append(('cat_compress_strings', autoai_libs.transformers.exportable.CompressStrings(activate_flag=True, compress_type='hash',\n                dtypes_list=['char_str', 'char_str', 'float_int_num', 'char_str', 'float_int_num', 'char_str', 'char_str', 'char_str', 'float_int_num'],\n                missing_values_reference_list=['?', '', '-', nan],\n                misslist_list=[[], [], [], [], [], [], [], [], []])))\n        preprocessor_features_categorical_steps.append(('cat_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n        preprocessor_features_categorical_steps.append(('cat_unknown_replacer', autoai_libs.transformers.exportable.NumpyReplaceUnknownValues(filling_values=nan,\n                     filling_values_list=[nan, nan, 100001, nan, 100001, nan, nan, nan, 100001],\n                     known_values_list=known_values_list,\n                     missing_values_reference_list=['?', '', '-', nan])))\n        preprocessor_features_categorical_steps.append(('boolean2float_transformer', autoai_libs.transformers.exportable.boolean2float(activate_flag=True)))\n        preprocessor_features_categorical_steps.append(('cat_imputer', autoai_libs.transformers.exportable.CatImputer(activate_flag=True, missing_values=nan,\n              sklearn_version_family='20', strategy='most_frequent')))\n        preprocessor_features_categorical_steps.append(('cat_encoder', autoai_libs.transformers.exportable.CatEncoder(activate_flag=True, categories='auto',\n              dtype=numpy.float64, encoding='ordinal',\n              handle_unknown='error', sklearn_version_family='20')))\n        preprocessor_features_categorical_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n        # assembling preprocessor_features_categorical_ Pipeline\n        preprocessor_features_categorical_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_categorical_steps)\n        preprocessor_features_transformer_list.append(('categorical', preprocessor_features_categorical_pipeline))\n        #\n        # composing steps for preprocessor_features_numeric Pipeline\n        #\n        preprocessor_features_numeric__input_metadata = None\n        preprocessor_features_numeric_steps=[]\n        preprocessor_features_numeric_steps.append(('num_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[3, 5, 6, 7, 8, 13])))\n        preprocessor_features_numeric_steps.append(('num_floatstr2float_transformer', autoai_libs.transformers.exportable.FloatStr2Float(activate_flag=True,\n                dtypes_list=['float_num', 'float_num', 'float_num', 'float_num', 'float_num', 'float_num'],\n                missing_values_reference_list=[])))\n        preprocessor_features_numeric_steps.append(('num_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n        preprocessor_features_numeric_steps.append(('num_imputer', autoai_libs.transformers.exportable.NumImputer(activate_flag=True, missing_values=nan, strategy='median')))\n        preprocessor_features_numeric_steps.append(('num_scaler', autoai_libs.transformers.exportable.OptStandardScaler(num_scaler_copy=None, num_scaler_with_mean=None,\n                 num_scaler_with_std=None, use_scaler_flag=False)))\n        preprocessor_features_numeric_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n        # assembling preprocessor_features_numeric_ Pipeline\n        preprocessor_features_numeric_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_numeric_steps)\n        preprocessor_features_transformer_list.append(('numeric', preprocessor_features_numeric_pipeline))\n        # assembling preprocessor_features_ FeatureUnion\n        preprocessor_features_pipeline = sklearn.pipeline.FeatureUnion(transformer_list=preprocessor_features_transformer_list)\n        preprocessor_steps.append(('features', preprocessor_features_pipeline))\n        preprocessor_steps.append(('permuter', autoai_libs.transformers.exportable.NumpyPermuteArray(axis=0,\n                 permutation_indices=[0, 1, 2, 4, 9, 10, 11, 12, 14, 3, 5, 6, 7, 8, 13])))\n        # assembling preprocessor_ Pipeline\n        preprocessor_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_steps)\n        steps.append(('preprocessor', preprocessor_pipeline))\n        steps.append(('estimator', sklearn.ensemble.forest.RandomForestClassifier(bootstrap=True, class_weight='balanced',\n                    criterion='gini', max_depth=None, max_features='auto',\n                    max_leaf_nodes=None, min_impurity_decrease=0.0,\n                    min_impurity_split=None, min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=10, n_jobs=4, oob_score=True, random_state=33,\n                    verbose=0, warm_start=False)))\n        # assembling  Pipeline\n        pipeline = sklearn.pipeline.Pipeline(steps=steps)\n\n        successful = True\n    except Exception as e:\n        estr = str(e)\n        if estr.startswith('name ') and estr.endswith(' is not defined'):\n            try:\n                import importlib\n                module_name = estr.split(\"'\")[1]\n                module = importlib.import_module(module_name)\n                globals().update({module_name: module})\n                print('import successful for ' + module_name)\n                failed_retries -= 1\n            except Exception as import_failure:\n                print('import of ' + module_name + ' failed with: ' + str(import_failure))\n                import subprocess\n                print('attempting pip install of ' + module_name)\n                process = subprocess.Popen('pip install ' + module_name, shell=True)\n                process.wait()\n                try:\n                    print('re-attempting import of ' + module_name)\n                    module = importlib.import_module(module_name)\n                    globals().update({module_name: module})\n                    print('import successful for ' + module_name)\n                    failed_retries -= 1\n                except Exception as import_or_installation_failure:\n                    print('failure installing and/or importing ' + module_name + ' error was: ' + str(\n                        import_or_installation_failure))\n                    raise (ModuleNotFoundError('Missing package in environment for ' + module_name +\n                        '? Try import and/or pip install manually?'))\n        elif type(e) is AttributeError:\n            if 'module ' in estr and ' has no attribute ' in estr:\n                pieces = estr.split(\"'\")\n                if len(pieces) == 5:\n                    try:\n                        import importlib\n                        print('re-attempting import of ' + pieces[3] + ' from ' + pieces[1])\n                        module = importlib.import_module('.' + pieces[3], pieces[1])\n                        failed_retries -= 1\n                    except:\n                        print('failed attempt to import ' + pieces[3])\n                        raise(e)\n                else:\n                    raise(e)\n        else:\n            raise(e)\nif successful:\n    print('Pipeline successfully instantiated')\nelse:\n    raise (ModuleNotFoundError('Remaining missing imports/packages in environment? Retry cell and/or try pip install manually?'))\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\ndata_source='replace_with_path_and_csv_filename'\ntarget_label_name = _input_metadata['target_label_name']\nlearning_type = _input_metadata['learning_type']\noptimization_metric = _input_metadata['optimization_metric']\nrandom_state = _input_metadata['random_state']\ncv_num_folds = _input_metadata['cv_num_folds']\nholdout_fraction = _input_metadata['holdout_fraction']\ndata_provenance = _input_metadata['data_provenance']\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import pandas as pd\ncsv_encodings=['UTF-8','Latin-1'] # supplement list of encodings as necessary for your data\ndf = None\nif type(data_provenance) is str: # try to load file specified\n    for encoding in csv_encodings:\n        try:\n            df = pd.read_csv(data_provenance, encoding=encoding) # your data file name here\n        except Exception as csv_exception:\n            print(csv_exception)\nif df is None: # try to load file from Cloud Object Storage\n    try:\n        data_location = data_provenance['input_data'][0]\n        print('data_location '+ str(data_location))\n        import boto3\n        session = boto3.session.Session()\n        cos = session.client(\n            service_name='s3',\n            aws_access_key_id=data_location['connection']['access_key_id'],\n            aws_secret_access_key=data_location['connection']['secret_access_key'],\n            endpoint_url=data_location['connection']['endpoint_url'],\n            verify=False\n        )\n        local_path = data_location['location']['path']\n        print('local_path ' + str(local_path))\n        cos.download_file(data_location['location']['bucket'],\n                     data_location['location']['path'],\n                     local_path)\n        for encoding in csv_encodings:\n            try:\n                df = pd.read_csv(local_path, encoding=encoding) # your data file name here\n            except Exception as csv_exception:\n                print(csv_exception)\n    except Exception as e:\n        print(e)\nif df is None:\n    raise(ValueError('Problem accessing or decoding csv data. May need csv_encoding string, or location or credential information to read dataframe from COS'))\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "target = target_label_name # your target name here\ndf_y = df[target]\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_X = df.drop(columns=[target])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.model_selection import train_test_split",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "if learning_type is None:\n    # When the problem type is not available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\n    # Caution:  This can mis-classify regression targets that can be expressed as integers as multiclass, in which case manually override the learning_type\n    from sklearn.utils.multiclass import type_of_target\n    if type_of_target(df_y.values) in ['multiclass', 'binary']:\n        learning_type = 'classification'\n    else:\n        learning_type = 'regression'\n    print('learning_type determined by type_of_target as:',learning_type)\nelse:\n    print('learning_type specified as:',learning_type)\nif learning_type == 'classification':\n    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\nelse:\n    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state)\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "pipeline.fit(X, y)\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y_pred = pipeline.predict(X_holdout)\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(y_holdout)\nprint(y_pred)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.metrics import get_scorer\nscorer = get_scorer(optimization_metric)\nscorer(pipeline, X_holdout, y_holdout)\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#delete CONTENTS of known_values_list before refitting, cloning or cross_validate-ing the pipeline, or previous values will be used.\nfor i in range(len(known_values_list)):\n    del known_values_list[0]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# preprocessor should see all data for cross_validate on the remaining steps to match autoai scores\npreprocessor = pipeline.steps[0][1]\npreprocessor.fit(X)\nX_prep = preprocessor.transform(X)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# remove preprocessor from pipeline\ndel pipeline.steps[0]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# cross_validate on remaining steps of pipeline\nfrom sklearn.model_selection import cross_validate\ncv_results = cross_validate(pipeline, X_prep, y, scoring={optimization_metric:scorer})\nimport numpy as np\nnp.mean(cv_results['test_' + optimization_metric])\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "cv_results",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}